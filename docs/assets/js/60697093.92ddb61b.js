"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[4239],{8074:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Features/voice-messages","title":"Voice Message Feature","description":"Text is great, but sometimes you need to hear someone\'s voice. That\'s why I added voice messages to the chat app - they add a more personal touch and are much quicker than typing out long messages.","source":"@site/docs-source/Features/voice-messages.md","sourceDirName":"Features","slug":"/Features/voice-messages","permalink":"/CustomChatApp/docs/Features/voice-messages","draft":false,"unlisted":false,"editUrl":"https://github.com/Emir2099/CustomChatApp/docs-source/Features/voice-messages.md","tags":[],"version":"current","lastUpdatedAt":1747596452000,"sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"File Sharing Feature","permalink":"/CustomChatApp/docs/Features/file-sharing"},"next":{"title":"User Profile Management","permalink":"/CustomChatApp/docs/Features/user-profile"}}');var o=s(4848),r=s(8453);const i={sidebar_position:7},a="Voice Message Feature",c={},d=[{value:"How Voice Messages Work",id:"how-voice-messages-work",level:2},{value:"Recording Implementation",id:"recording-implementation",level:2},{value:"Sending Voice Messages",id:"sending-voice-messages",level:2},{value:"Voice Message Component",id:"voice-message-component",level:2},{value:"Styling the Voice Messages",id:"styling-the-voice-messages",level:2},{value:"Integrating with Message Input",id:"integrating-with-message-input",level:2},{value:"Technical Challenges",id:"technical-challenges",level:2},{value:"Browser Compatibility",id:"browser-compatibility",level:3},{value:"Audio Format Compatibility",id:"audio-format-compatibility",level:3},{value:"File Size Concerns",id:"file-size-concerns",level:3},{value:"User Experience Considerations",id:"user-experience-considerations",level:2},{value:"Future Improvements",id:"future-improvements",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-message-feature",children:"Voice Message Feature"})}),"\n",(0,o.jsx)(n.p,{children:"Text is great, but sometimes you need to hear someone's voice. That's why I added voice messages to the chat app - they add a more personal touch and are much quicker than typing out long messages."}),"\n",(0,o.jsx)(n.h2,{id:"how-voice-messages-work",children:"How Voice Messages Work"}),"\n",(0,o.jsx)(n.p,{children:"The voice message system follows this flow:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"User clicks the microphone button to start recording"}),"\n",(0,o.jsx)(n.li,{children:"Audio is recorded using the Web Audio API"}),"\n",(0,o.jsx)(n.li,{children:"When finished, the audio is converted to a blob"}),"\n",(0,o.jsx)(n.li,{children:"The blob is encoded as base64 and sent to Firebase"}),"\n",(0,o.jsx)(n.li,{children:"In the chat, users can play back the voice messages"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"recording-implementation",children:"Recording Implementation"}),"\n",(0,o.jsx)(n.p,{children:"I implemented the recording functionality using the MediaRecorder API:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:"// src/components/chat/VoiceRecorder.jsx\nimport { useState, useRef, useEffect } from 'react';\nimport { useChat } from '../../contexts/ChatContext';\nimport styles from './VoiceRecorder.module.css';\n\nexport default function VoiceRecorder({ onClose, replyToId = null }) {\n  const [isRecording, setIsRecording] = useState(false);\n  const [recordingTime, setRecordingTime] = useState(0);\n  const [audioBlob, setAudioBlob] = useState(null);\n  \n  const { sendVoiceMessage } = useChat();\n  \n  const mediaRecorderRef = useRef(null);\n  const audioChunksRef = useRef([]);\n  const timerRef = useRef(null);\n  \n  // Set up recording\n  const startRecording = async () => {\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n      \n      const mediaRecorder = new MediaRecorder(stream);\n      mediaRecorderRef.current = mediaRecorder;\n      \n      audioChunksRef.current = [];\n      \n      mediaRecorder.ondataavailable = (e) => {\n        if (e.data.size > 0) {\n          audioChunksRef.current.push(e.data);\n        }\n      };\n      \n      mediaRecorder.onstop = () => {\n        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });\n        setAudioBlob(audioBlob);\n        \n        // Stop all audio tracks to release the microphone\n        stream.getTracks().forEach(track => track.stop());\n      };\n      \n      // Start recording\n      mediaRecorder.start();\n      setIsRecording(true);\n      \n      // Start timer\n      timerRef.current = setInterval(() => {\n        setRecordingTime(prev => prev + 1);\n      }, 1000);\n      \n    } catch (error) {\n      console.error('Error accessing microphone:', error);\n    }\n  };\n  \n  const stopRecording = () => {\n    if (mediaRecorderRef.current && isRecording) {\n      mediaRecorderRef.current.stop();\n      setIsRecording(false);\n      \n      // Clear timer\n      clearInterval(timerRef.current);\n    }\n  };\n  \n  // Clean up on unmount\n  useEffect(() => {\n    return () => {\n      if (timerRef.current) {\n        clearInterval(timerRef.current);\n      }\n      \n      if (mediaRecorderRef.current && isRecording) {\n        mediaRecorderRef.current.stop();\n      }\n    };\n  }, [isRecording]);\n  \n  // Format seconds to mm:ss\n  const formatTime = (seconds) => {\n    const mins = Math.floor(seconds / 60).toString().padStart(2, '0');\n    const secs = (seconds % 60).toString().padStart(2, '0');\n    return `${mins}:${secs}`;\n  };\n  \n  // Send the voice message\n  const handleSend = async () => {\n    if (!audioBlob) return;\n    \n    try {\n      await sendVoiceMessage(audioBlob, recordingTime, replyToId);\n      onClose();\n    } catch (error) {\n      console.error('Error sending voice message:', error);\n    }\n  };\n  \n  // Cancel recording\n  const handleCancel = () => {\n    if (isRecording) {\n      stopRecording();\n    }\n    onClose();\n  };\n  \n  return (\n    <div className={styles.voiceRecorder}>\n      <div className={styles.header}>\n        {isRecording ? 'Recording...' : audioBlob ? 'Preview' : 'Voice Message'}\n      </div>\n      \n      <div className={styles.recordingInfo}>\n        {isRecording ? (\n          <div className={styles.recordingIndicator}>\n            <span className={styles.recordingDot} />\n            <span>{formatTime(recordingTime)}</span>\n          </div>\n        ) : audioBlob ? (\n          <audio \n            src={URL.createObjectURL(audioBlob)} \n            controls \n            className={styles.audioPreview}\n          />\n        ) : (\n          <div className={styles.instructions}>\n            Press the button to start recording\n          </div>\n        )}\n      </div>\n      \n      <div className={styles.actions}>\n        {isRecording ? (\n          <button \n            className={styles.stopButton}\n            onClick={stopRecording}\n          >\n            Stop Recording\n          </button>\n        ) : audioBlob ? (\n          <>\n            <button \n              className={styles.cancelButton}\n              onClick={handleCancel}\n            >\n              Cancel\n            </button>\n            <button \n              className={styles.sendButton}\n              onClick={handleSend}\n            >\n              Send Voice Message\n            </button>\n          </>\n        ) : (\n          <>\n            <button \n              className={styles.cancelButton}\n              onClick={handleCancel}\n            >\n              Cancel\n            </button>\n            <button \n              className={styles.recordButton}\n              onClick={startRecording}\n            >\n              Start Recording\n            </button>\n          </>\n        )}\n      </div>\n    </div>\n  );\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"sending-voice-messages",children:"Sending Voice Messages"}),"\n",(0,o.jsx)(n.p,{children:"In the ChatContext, I added a dedicated function for sending voice messages:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:"// In ChatContext.jsx\nconst sendVoiceMessage = async (audioBlob, duration, replyToId = null) => {\n  if (!currentChat?.id || !user?.uid || !audioBlob) return;\n  \n  try {\n    // Convert blob to base64\n    const reader = new FileReader();\n    const base64 = await new Promise((resolve, reject) => {\n      reader.onload = () => resolve(reader.result);\n      reader.onerror = () => reject(reader.error);\n      reader.readAsDataURL(audioBlob);\n    });\n    \n    // Create message\n    const messageRef = push(ref(db, `messages/${currentChat.id}`));\n    \n    const message = {\n      id: messageRef.key,\n      type: 'voice',\n      voiceData: base64,\n      duration: duration, // in seconds\n      sender: user.uid,\n      senderName: user.displayName,\n      senderPhotoURL: user.photoURL,\n      timestamp: serverTimestamp(),\n      readBy: {\n        [user.uid]: serverTimestamp()\n      },\n      replyTo: replyToId\n    };\n    \n    await set(messageRef, message);\n    \n    // Update last message in chat\n    await update(ref(db, `chats/${currentChat.id}`), {\n      lastMessage: {\n        content: '\ud83c\udfa4 Voice message',\n        sender: user.uid,\n        senderName: user.displayName,\n        timestamp: serverTimestamp()\n      },\n      lastMessageTime: serverTimestamp()\n    });\n    \n    return messageRef.key;\n  } catch (error) {\n    console.error('Error sending voice message:', error);\n    return null;\n  }\n};\n"})}),"\n",(0,o.jsx)(n.h2,{id:"voice-message-component",children:"Voice Message Component"}),"\n",(0,o.jsx)(n.p,{children:"For displaying voice messages in the chat, I created a dedicated component:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:"// src/components/chat/VoiceMessage.jsx\nimport { useState, useRef } from 'react';\nimport styles from './VoiceMessage.module.css';\n\nexport default function VoiceMessage({ message }) {\n  const [isPlaying, setIsPlaying] = useState(false);\n  const [currentTime, setCurrentTime] = useState(0);\n  \n  const audioRef = useRef(null);\n  \n  // Format duration in seconds to mm:ss\n  const formatDuration = (seconds) => {\n    const mins = Math.floor(seconds / 60).toString().padStart(2, '0');\n    const secs = (seconds % 60).toString().padStart(2, '0');\n    return `${mins}:${secs}`;\n  };\n  \n  // Handle play/pause\n  const togglePlay = () => {\n    if (audioRef.current) {\n      if (isPlaying) {\n        audioRef.current.pause();\n      } else {\n        audioRef.current.play();\n      }\n    }\n  };\n  \n  // Audio event handlers\n  const handlePlay = () => setIsPlaying(true);\n  const handlePause = () => setIsPlaying(false);\n  const handleEnded = () => setIsPlaying(false);\n  const handleTimeUpdate = () => {\n    if (audioRef.current) {\n      setCurrentTime(Math.floor(audioRef.current.currentTime));\n    }\n  };\n  \n  return (\n    <div className={styles.voiceMessage}>\n      <audio\n        ref={audioRef}\n        src={message.voiceData}\n        onPlay={handlePlay}\n        onPause={handlePause}\n        onEnded={handleEnded}\n        onTimeUpdate={handleTimeUpdate}\n        preload=\"metadata\"\n      />\n      \n      <button \n        className={`${styles.playButton} ${isPlaying ? styles.playing : ''}`}\n        onClick={togglePlay}\n        aria-label={isPlaying ? 'Pause voice message' : 'Play voice message'}\n      >\n        {isPlaying ? '\u275a\u275a' : '\u25b6'}\n      </button>\n      \n      <div className={styles.waveform}>\n        <div \n          className={styles.progress}\n          style={{ \n            width: message.duration > 0 \n              ? `${(currentTime / message.duration) * 100}%` \n              : '0%' \n          }}\n        />\n      </div>\n      \n      <div className={styles.duration}>\n        {formatDuration(isPlaying ? currentTime : message.duration)}\n      </div>\n    </div>\n  );\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"styling-the-voice-messages",children:"Styling the Voice Messages"}),"\n",(0,o.jsx)(n.p,{children:"I wanted the voice messages to have a distinctive look:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-css",children:"/* VoiceMessage.module.css */\n.voiceMessage {\n  display: flex;\n  align-items: center;\n  gap: 8px;\n  width: 200px;\n}\n\n.playButton {\n  width: 32px;\n  height: 32px;\n  border-radius: 50%;\n  background-color: #0084ff;\n  color: white;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  border: none;\n  cursor: pointer;\n  font-size: 12px;\n}\n\n.playing {\n  background-color: #555;\n}\n\n.waveform {\n  flex: 1;\n  height: 24px;\n  background-color: rgba(0, 0, 0, 0.1);\n  border-radius: 12px;\n  position: relative;\n  overflow: hidden;\n}\n\n.progress {\n  position: absolute;\n  left: 0;\n  top: 0;\n  height: 100%;\n  background-color: rgba(0, 132, 255, 0.3);\n  transition: width 0.1s linear;\n}\n\n.duration {\n  font-size: 12px;\n  color: #555;\n  min-width: 40px;\n  text-align: right;\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"integrating-with-message-input",children:"Integrating with Message Input"}),"\n",(0,o.jsx)(n.p,{children:"I added a microphone button to the message input to trigger voice recording:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:'// In MessageInput.jsx\nconst [isRecording, setIsRecording] = useState(false);\n\nconst handleVoiceButtonClick = () => {\n  setIsRecording(true);\n};\n\n// In the JSX\n<div className={styles.messageInputContainer}>\n  {/* Text input */}\n  <textarea\n    // ...\n  />\n  \n  {/* Voice button */}\n  <button\n    type="button"\n    className={styles.voiceButton}\n    onClick={handleVoiceButtonClick}\n    aria-label="Record voice message"\n  >\n    \ud83c\udfa4\n  </button>\n  \n  {/* Voice recorder modal */}\n  {isRecording && (\n    <div className={styles.recorderModal}>\n      <VoiceRecorder \n        onClose={() => setIsRecording(false)}\n        replyToId={replyingTo?.id}\n      />\n    </div>\n  )}\n</div>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"technical-challenges",children:"Technical Challenges"}),"\n",(0,o.jsx)(n.h3,{id:"browser-compatibility",children:"Browser Compatibility"}),"\n",(0,o.jsx)(n.p,{children:"One of the biggest challenges was browser compatibility. The MediaRecorder API isn't supported in all browsers, so I had to add a fallback:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:"// Check for MediaRecorder support\nconst isRecordingSupported = () => {\n  return 'MediaRecorder' in window;\n};\n\n// In the component\nuseEffect(() => {\n  if (!isRecordingSupported()) {\n    setError('Voice recording is not supported in your browser.');\n  }\n}, []);\n\n// In the render\n{!isRecordingSupported() ? (\n  <div className={styles.notSupported}>\n    Voice recording is not supported in your browser.\n    Try using Chrome or Firefox for this feature.\n  </div>\n) : (\n  // Normal recorder UI\n  // ...\n)}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"audio-format-compatibility",children:"Audio Format Compatibility"}),"\n",(0,o.jsx)(n.p,{children:"Different browsers support different audio formats, which was tricky:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:"// Get the appropriate MIME type based on browser support\nconst getMimeType = () => {\n  const types = [\n    'audio/webm;codecs=opus',\n    'audio/webm',\n    'audio/ogg;codecs=opus',\n    'audio/mp4'\n  ];\n  \n  for (const type of types) {\n    if (MediaRecorder.isTypeSupported(type)) {\n      return type;\n    }\n  }\n  \n  return '';\n};\n\n// Use the supported MIME type when creating the MediaRecorder\nconst mediaRecorder = new MediaRecorder(stream, { \n  mimeType: getMimeType() \n});\n"})}),"\n",(0,o.jsx)(n.h3,{id:"file-size-concerns",children:"File Size Concerns"}),"\n",(0,o.jsx)(n.p,{children:"Voice messages can get large, so I had to implement some limits:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:"// In VoiceRecorder.jsx\nconst MAX_RECORDING_TIME = 60; // 1 minute max\n\nuseEffect(() => {\n  if (recordingTime >= MAX_RECORDING_TIME) {\n    stopRecording();\n  }\n}, [recordingTime]);\n"})}),"\n",(0,o.jsx)(n.p,{children:"And in the ChatContext:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-jsx",children:"// In sendVoiceMessage\nif (base64.length > 1024 * 1024) { // If larger than 1MB\n  // Compress or reject the audio\n  throw new Error('Voice message too large');\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"user-experience-considerations",children:"User Experience Considerations"}),"\n",(0,o.jsx)(n.p,{children:"I wanted to make recording voice messages as intuitive as possible:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Clear Feedback"})," - Visual recording indicator and timer"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Preview Before Sending"})," - Listen to your recording before sending"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cancel Option"})," - Easy way to discard a recording"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Playback Controls"})," - Simple play/pause with progress indicator"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Duration Display"})," - Show how long the message is"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"future-improvements",children:"Future Improvements"}),"\n",(0,o.jsx)(n.p,{children:"If I had more time, I'd enhance the voice message feature with:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Waveform Visualization"})," - Show the actual audio waveform"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speed Controls"})," - Allow playback at different speeds (0.5x, 1x, 1.5x, 2x)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transcription"})," - Add automatic speech-to-text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Noise Reduction"})," - Improve audio quality with filters"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Better Compression"})," - Reduce file size while maintaining quality"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The voice message feature adds a whole new dimension to the chat experience. I've found it particularly useful for quickly explaining complex ideas or adding a personal touch to conversations."})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>a});var t=s(6540);const o={},r=t.createContext(o);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);